{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fine-tuning the library models for causal language modeling (GPT, GPT-2, CTRL, ...) on a text file or a dataset.\n",
    "Here is the full list of checkpoints on the hub that can be fine-tuned by this script:\n",
    "https://huggingface.co/models?filter=text-generation\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "    Code here heavily burrows from https://github.com/locuslab/wanda/tree/main\n",
    "\"\"\"\n",
    "# You can also adapt this script on your own causal language modeling task. Pointers for this are left as comments.\n",
    "from transformers.pytorch_utils import  find_pruneable_heads_and_indices, prune_linear_layer\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from itertools import chain\n",
    "from typing import Optional, List \n",
    "import pdb\n",
    "import pickle as pkl\n",
    "import gc\n",
    "import time\n",
    "from peft import PeftModel\n",
    "# Uncomment this out if running the Eleuther evaluation harness\n",
    "# import lm_eval\n",
    "# from lm_eval import evaluator\n",
    "\n",
    "import datasets\n",
    "# import evaluate\n",
    "import torch\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_FOR_CAUSAL_LM_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    is_torch_tpu_available,\n",
    "    set_seed\n",
    ")\n",
    "from transformers.testing_utils import CaptureLogger\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version, send_example_telemetry\n",
    "from transformers.utils.versions import require_version\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    # prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from evaluate_ppl import evaluate_ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_to_use = 'cuda:5'\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.54s/it]\n"
     ]
    }
   ],
   "source": [
    "target_model_name = 'meta-llama/Llama-2-7b-hf'\n",
    "tokenizer_name = 'meta-llama/Llama-2-7b-hf'\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(target_model_name, torch_dtype=torch.float16, device_map=device_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for PeftModelForCausalLM:\n\tsize mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.0.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.0.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.0.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 1920]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.0.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5514, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.0.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5514, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.0.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5514]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.1.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.1.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.1.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 2048]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.1.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5542, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.1.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5542, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.1.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5542]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2560, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.2.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2560, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.2.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2560, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.2.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 2560]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.2.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5565, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.2.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5565, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.2.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5565]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.3.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.3.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.3.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 1920]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.3.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5569, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.3.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5569, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.3.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5569]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.4.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.4.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.4.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 1920]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.4.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5570, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.4.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5570, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.4.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5570]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2176, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.5.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2176, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.5.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2176, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.5.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 2176]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.5.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5472, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.5.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5472, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.5.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5472]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.6.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.6.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.6.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 2048]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.6.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5544, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.6.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5544, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.6.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5544]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.7.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.7.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.7.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 2048]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.7.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5535, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.7.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5535, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.7.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5535]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2432, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.8.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2432, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.8.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2432, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.8.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 2432]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.8.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5584, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.8.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5584, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.8.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5584]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.9.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.9.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.9.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 1920]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.9.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5509, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.9.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5509, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.9.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5509]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.10.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.10.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.10.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 1920]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.10.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5569, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.10.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5569, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.10.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5569]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.11.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.11.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.11.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 2048]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.11.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5513, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.11.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5513, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.11.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5513]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1664, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.12.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1664, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.12.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1664, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.12.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 1664]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.12.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5528, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.12.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5528, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.12.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5528]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2432, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.13.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2432, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.13.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2432, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.13.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 2432]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.13.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5565, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.13.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5565, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.13.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5565]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2304, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.14.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2304, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.14.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2304, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.14.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 2304]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.14.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5549, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.14.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5549, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.14.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5549]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.15.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.15.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.15.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 1920]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.15.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5561, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.15.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5561, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.15.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5561]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1792, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.16.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1792, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.16.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1792, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.16.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 1792]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.16.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5543, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.16.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5543, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.16.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5543]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.17.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.17.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.17.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 1920]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.17.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5475, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.17.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5475, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.17.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5475]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2176, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.18.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2176, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.18.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2176, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.18.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 2176]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.18.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5567, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.18.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5567, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.18.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5567]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.19.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.19.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.19.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 2048]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.19.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5514, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.19.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5514, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.19.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5514]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2560, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.20.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2560, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.20.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2560, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.20.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 2560]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.20.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5482, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.20.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5482, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.20.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5482]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1408, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.21.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1408, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.21.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1408, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.21.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 1408]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.21.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5572, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.21.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5572, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.21.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5572]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.22.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2176, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.22.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2176, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.22.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2176, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.22.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 2176]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.22.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5548, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.22.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5548, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.22.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5548]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.23.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.23.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.23.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.23.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 2048]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.23.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5506, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.23.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5506, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.23.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5506]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.24.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.24.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.24.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.24.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 2048]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.24.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5521, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.24.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5521, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.24.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5521]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.25.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1792, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.25.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1792, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.25.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1792, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.25.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 1792]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.25.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5532, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.25.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5532, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.25.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5532]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.26.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1792, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.26.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1792, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.26.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1792, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.26.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 1792]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.26.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5497, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.26.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5497, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.26.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5497]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.27.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.27.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.27.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.27.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 2048]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.27.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5570, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.27.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5570, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.27.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5570]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.28.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.28.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.28.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.28.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 1920]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.28.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5561, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.28.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5561, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.28.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5561]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.29.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.29.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.29.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.29.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 1920]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.29.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5537, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.29.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5537, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.29.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5537]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.30.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1792, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.30.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1792, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.30.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1792, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.30.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 1792]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.30.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5523, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.30.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5523, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.30.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5523]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.31.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.31.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.31.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.31.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 2048]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.31.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5505, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.31.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5505, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.31.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5505]) from checkpoint, the shape in current model is torch.Size([64, 11008]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m adapter_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/vashistt/Desktop/anlp-project/finetuned_model_prune_c4_ft_wiki/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# model.load_adapter(adapter_name)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# model.set_active_adapters('prune-c4_ft_wiki_adapter')\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPeftModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprune_c4_ft_wiki_adapter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ENTER/envs/griffin/lib/python3.12/site-packages/peft/peft_model.py:354\u001b[0m, in \u001b[0;36mPeftModel.from_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    353\u001b[0m     model \u001b[38;5;241m=\u001b[39m MODEL_TYPE_TO_PEFT_MODEL_MAPPING[config\u001b[38;5;241m.\u001b[39mtask_type](model, config, adapter_name)\n\u001b[0;32m--> 354\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_trainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_trainable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/ENTER/envs/griffin/lib/python3.12/site-packages/peft/peft_model.py:698\u001b[0m, in \u001b[0;36mPeftModel.load_adapter\u001b[0;34m(self, model_id, adapter_name, is_trainable, **kwargs)\u001b[0m\n\u001b[1;32m    695\u001b[0m adapters_weights \u001b[38;5;241m=\u001b[39m load_peft_weights(model_id, device\u001b[38;5;241m=\u001b[39mtorch_device, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhf_hub_download_kwargs)\n\u001b[1;32m    697\u001b[0m \u001b[38;5;66;03m# load the weights into the model\u001b[39;00m\n\u001b[0;32m--> 698\u001b[0m load_result \u001b[38;5;241m=\u001b[39m \u001b[43mset_peft_model_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapters_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    700\u001b[0m     (\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_device_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    701\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhf_device_map\u001b[38;5;241m.\u001b[39mvalues())\u001b[38;5;241m.\u001b[39mintersection({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m})) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m ):\n\u001b[1;32m    704\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/ENTER/envs/griffin/lib/python3.12/site-packages/peft/utils/save_and_load.py:241\u001b[0m, in \u001b[0;36mset_peft_model_state_dict\u001b[0;34m(model, peft_model_state_dict, adapter_name)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m load_result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpeft_model_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mis_prompt_learning:\n\u001b[1;32m    243\u001b[0m     model\u001b[38;5;241m.\u001b[39mprompt_encoder[adapter_name]\u001b[38;5;241m.\u001b[39membedding\u001b[38;5;241m.\u001b[39mload_state_dict(\n\u001b[1;32m    244\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m: peft_model_state_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m]}, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     )\n",
      "File \u001b[0;32m~/ENTER/envs/griffin/lib/python3.12/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for PeftModelForCausalLM:\n\tsize mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.0.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.0.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.0.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 1920]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.0.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5514, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.0.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5514, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.0.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5514]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.1.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.1.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.1.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 2048]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.1.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5542, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.1.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5542, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.1.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5542]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2560, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.2.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2560, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.2.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2560, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.2.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 2560]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.2.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5565, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.2.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5565, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.2.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5565]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.3.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.3.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.3.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 1920]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.3.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5569, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.3.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5569, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.3.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5569]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.4.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.4.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.4.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 1920]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.4.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5570, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.4.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5570, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.4.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5570]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2176, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.5.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2176, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.5.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2176, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.5.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 2176]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.5.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5472, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.5.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5472, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.5.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5472]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.6.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.6.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.6.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 2048]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.6.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5544, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.6.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5544, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.6.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5544]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.7.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.7.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.7.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 2048]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.7.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5535, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.7.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5535, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.7.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5535]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2432, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.8.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2432, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.8.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2432, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.8.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 2432]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.8.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5584, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.8.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5584, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.8.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5584]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.9.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.9.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.9.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 1920]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.9.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5509, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.9.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5509, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.9.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5509]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.10.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.10.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.10.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 1920]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.10.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5569, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.10.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5569, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.10.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5569]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.11.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.11.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.11.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 2048]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.11.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5513, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.11.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5513, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.11.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5513]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1664, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.12.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1664, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.12.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1664, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.12.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 1664]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.12.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5528, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.12.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5528, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.12.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5528]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2432, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.13.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2432, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.13.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2432, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.13.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 2432]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.13.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5565, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.13.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5565, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.13.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5565]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2304, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.14.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2304, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.14.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2304, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.14.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 2304]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.14.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5549, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.14.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5549, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.14.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5549]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.15.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.15.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.15.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 1920]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.15.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5561, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.15.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5561, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.15.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5561]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1792, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.16.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1792, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.16.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1792, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.16.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 1792]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.16.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5543, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.16.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5543, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.16.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5543]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.17.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.17.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.17.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 1920]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.17.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5475, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.17.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5475, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.17.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5475]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2176, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.18.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2176, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.18.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2176, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.18.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 2176]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.18.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5567, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.18.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5567, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.18.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5567]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.19.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.19.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.19.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 2048]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.19.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5514, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.19.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5514, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.19.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5514]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2560, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.20.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2560, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.20.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2560, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.20.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 2560]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.20.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5482, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.20.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5482, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.20.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5482]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1408, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.21.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1408, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.21.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1408, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.21.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 1408]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.21.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5572, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.21.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5572, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.21.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5572]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.22.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2176, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.22.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2176, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.22.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2176, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.22.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 2176]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.22.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5548, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.22.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5548, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.22.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5548]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.23.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.23.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.23.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.23.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 2048]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.23.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5506, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.23.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5506, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.23.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5506]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.24.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.24.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.24.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.24.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 2048]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.24.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5521, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.24.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5521, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.24.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5521]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.25.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1792, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.25.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1792, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.25.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1792, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.25.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 1792]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.25.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5532, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.25.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5532, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.25.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5532]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.26.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1792, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.26.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1792, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.26.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1792, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.26.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 1792]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.26.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5497, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.26.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5497, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.26.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5497]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.27.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.27.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.27.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.27.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 2048]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.27.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5570, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.27.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5570, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.27.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5570]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.28.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.28.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.28.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.28.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 1920]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.28.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5561, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.28.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5561, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.28.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5561]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.29.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.29.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.29.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1920, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.29.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 1920]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.29.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5537, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.29.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5537, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.29.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5537]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.30.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1792, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.30.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1792, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.30.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([1792, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.30.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 1792]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.30.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5523, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.30.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5523, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.30.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5523]) from checkpoint, the shape in current model is torch.Size([64, 11008]).\n\tsize mismatch for base_model.model.model.layers.31.self_attn.q_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.31.self_attn.k_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.31.self_attn.v_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).\n\tsize mismatch for base_model.model.model.layers.31.self_attn.o_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 2048]) from checkpoint, the shape in current model is torch.Size([64, 4096]).\n\tsize mismatch for base_model.model.model.layers.31.mlp.gate_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5505, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.31.mlp.up_proj.lora_B.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([5505, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).\n\tsize mismatch for base_model.model.model.layers.31.mlp.down_proj.lora_A.prune_c4_ft_wiki_adapter.weight: copying a param with shape torch.Size([64, 5505]) from checkpoint, the shape in current model is torch.Size([64, 11008])."
     ]
    }
   ],
   "source": [
    "adapter_name = '/home/vashistt/Desktop/anlp-project/finetuned_model_prune_c4_ft_wiki/'\n",
    "# model.load_adapter(adapter_name)\n",
    "# model.set_active_adapters('prune-c4_ft_wiki_adapter')\n",
    "model = PeftModel.from_pretrained(model, adapter_name, adapter_name=\"prune_c4_ft_wiki_adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bonsai_pruning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
