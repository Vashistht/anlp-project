# Create and activate new conda environment
conda create -n prune_llm mamba -c conda-forge
source activate prune_llm

# Install PyTorch, torchvision, torchaudio, and other libraries
pip3 install ipykernel scipy matplotlib

### Torch audio and vision are not needed so you can skip this 
mamba install -c pytorch -c conda-forge -c defaults ipykernel torchaudio torchvision scipy matplotlib

# Install additional Python packages
pip3 install numpy wandb accelerate chardet==5.2.0 datasets huggingface-hub transformers pandas plotly torch tqdm urllib3 sentencepiece

# Upgrade huggingface_hub for CLI support
pip3 install -U "huggingface_hub[cli]"

# For finetuning
pip3 install evaluate==0.4.1 peft==0.6.2

# Register environment with Jupyter
python -m ipykernel install --user --name=prune_llm

# Login to Hugging Face CLI
huggingface-cli login --token YOUR_HF_API_TOKEN

# Running the commands

## Prune Sheared LLaMA 2-7B
CUDA_VISIBLE_DEVICES=0 python3 main.py --model princeton-nlp/Sheared-LLaMA-2.7B --dataset wikitext2 --sparsity_ratio 0.5 --wandb_project_name pruning-sheared-llama2-wikitext --masks_per_iter 100 --nsamples 8 --save outdir --prune_frac 0.2 --bsz 1 --prune_method wanda

## Prune LLaMA 2 7B
CUDA_VISIBLE_DEVICES=0 python3 main.py --model meta-llama/Llama-2-7b-hf --dataset wikitext2 --sparsity_ratio 0.5 --wandb_project_name pruning-llama2-wikitext --masks_per_iter 100 --nsamples 8 --save outdir --prune_frac 0.2 --bsz 1 --prune_method wanda

# Prune Sheared Llama-1 3B
CUDA_VISIBLE_DEVICES=0 python3 main.py --model princeton-nlp/Sheared-LLaMA-1.3B --dataset wikitext2 --sparsity_ratio 0.5 --wandb_project_name pruning-llama1-3B-wikitext --masks_per_iter 100 --nsamples 8 --save outdir --prune_frac 0.2 --bsz 1 --prune_method wanda


# JackFram/llama-68m
CUDA_VISIBLE_DEVICES=0 python3 main.py --model JackFram/llama-68m --dataset wikitext2 --sparsity_ratio 0.5 --wandb_project_name pruning-sheared-llama2-wikitext --masks_per_iter 100 --nsamples 8 --save outdir --prune_frac 0.2 --bsz 1 --prune_method wanda


## Tiny Llama (older)
<!---
CUDA_VISIBLE_DEVICES=0  python3 main.py --model TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T --dataset wikitext2 --sparsity_ratio 0.5 --wandb_project_name pruning-phi-wikitext --masks_per_iter 200 --nsamples 32 --save outdir  --prune_frac 0.05 --bsz 1 --prune_method wanda
-->

