# Setup
## Please verify with python, import torch, torch.cuda.is_available(), torch.cuda.device_count() to see if it is the right version

conda create -n prune_llm mamba -c conda-forge
source activate prune_llm

mamba install  -c pytorch -c conda-forge -c defaults ipykernel torchaudio torchvision scipy matplotlib

pip3 install numpy wandb accelerate chardet==5.2.0 datasets huggingface-hub transformers pandas plotly torch tqdm urllib3 sentencepiece 

pip3 install -U "huggingface_hub[cli]"

python -m ipykernel install --user --name=prune_llm


# Running the commands
## this is for phi-1.5

huggingface-cli login --token (add hf api token here)

CUDA_VISIBLE_DEVICES=0  python main.py --model meta-llama/Llama-2-7b-hf --dataset wikitext2 --sparsity_ratio 0.5 --wandb_project_name pruning-llama2-wikitext --masks_per_iter 200 --nsamples 8 --save outdir  --prune_frac 0.2 --bsz 1 --prune_method wanda


<!---
CUDA_VISIBLE_DEVICES=0  python main.py --model TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T --dataset wikitext2 --sparsity_ratio 0.5 --wandb_project_name pruning-phi-wikitext --masks_per_iter 200 --nsamples 32 --save outdir  --prune_frac 0.05 --bsz 1 --prune_method wanda
-->

# trying sheared llama
CUDA_VISIBLE_DEVICES=0  python main.py --model princeton-nlp/Sheared-LLaMA-2.7B --dataset wikitext2 --sparsity_ratio 0.5 --wandb_project_name pruning-sheared-llama2-wikitext --masks_per_iter 200 --nsamples 8 --save outdir  --prune_frac 0.2 --bsz 1 --prune_method wanda


