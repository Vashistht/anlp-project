04/29/2024 00:32:41 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
04/29/2024 00:32:41 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/vashistt/Desktop/anlp-project/finetuned_model/c4_masks-100/runs/Apr29_00-32-41_lovelace.ece.local.cmu.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_hf,
optim_args=None,
output_dir=/home/vashistt/Desktop/anlp-project/finetuned_model/c4_masks-100,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=1,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=/home/vashistt/Desktop/anlp-project/finetuned_model/c4_masks-100,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
[INFO|configuration_utils.py:668] 2024-04-29 00:32:41,564 >> loading configuration file config.json from cache at /home/vashistt/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/config.json
[INFO|configuration_utils.py:720] 2024-04-29 00:32:41,565 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:1809] 2024-04-29 00:32:41,633 >> loading file tokenizer.model from cache at /home/vashistt/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer.model
[INFO|tokenization_utils_base.py:1809] 2024-04-29 00:32:41,633 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1809] 2024-04-29 00:32:41,633 >> loading file special_tokens_map.json from cache at /home/vashistt/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/special_tokens_map.json
[INFO|tokenization_utils_base.py:1809] 2024-04-29 00:32:41,633 >> loading file tokenizer_config.json from cache at /home/vashistt/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer_config.json
[INFO|configuration_utils.py:668] 2024-04-29 00:32:41,679 >> loading configuration file config.json from cache at /home/vashistt/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/config.json
[INFO|configuration_utils.py:720] 2024-04-29 00:32:41,680 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.28.0",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|modeling_utils.py:2534] 2024-04-29 00:32:41,683 >> loading weights file model.safetensors from cache at /home/vashistt/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/model.safetensors.index.json
[INFO|modeling_utils.py:1176] 2024-04-29 00:32:41,684 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.
[INFO|configuration_utils.py:575] 2024-04-29 00:32:41,684 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.28.0"
}

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.58s/it]
[INFO|modeling_utils.py:3190] 2024-04-29 00:32:45,267 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3198] 2024-04-29 00:32:45,267 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:537] 2024-04-29 00:32:45,307 >> loading configuration file generation_config.json from cache at /home/vashistt/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/generation_config.json
[INFO|modeling_utils.py:2839] 2024-04-29 00:32:45,308 >> Generation config file not found, using a generation config created from the model config.
File path:  /home/vashistt/Desktop/anlp-project/finetuned_model/c4_masks-100/output_all_others.txt
Num params = :  6476271616
File created successfully.
04/29/2024 00:32:45 - INFO - __main__ - *** Evaluate ***
Overwrite dataset info from restored data version if exists.
04/29/2024 00:32:46 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/vashistt/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
04/29/2024 00:32:46 - INFO - datasets.info - Loading Dataset info from /home/vashistt/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
Found cached dataset wikitext (/home/vashistt/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
04/29/2024 00:32:46 - INFO - datasets.builder - Found cached dataset wikitext (/home/vashistt/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)
Loading Dataset info from /home/vashistt/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
04/29/2024 00:32:46 - INFO - datasets.info - Loading Dataset info from /home/vashistt/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3
  0%|          | 0/84 [00:00<?, ?it/s]  1%|          | 1/84 [00:01<02:20,  1.70s/it]  2%|▏         | 2/84 [00:02<01:23,  1.02s/it]  4%|▎         | 3/84 [00:03<01:30,  1.11s/it]  5%|▍         | 4/84 [00:04<01:32,  1.16s/it]  6%|▌         | 5/84 [00:05<01:33,  1.18s/it]  7%|▋         | 6/84 [00:07<01:33,  1.20s/it]  8%|▊         | 7/84 [00:08<01:32,  1.21s/it] 10%|▉         | 8/84 [00:09<01:32,  1.21s/it] 11%|█         | 9/84 [00:10<01:31,  1.22s/it] 12%|█▏        | 10/84 [00:12<01:30,  1.22s/it] 13%|█▎        | 11/84 [00:13<01:29,  1.22s/it] 14%|█▍        | 12/84 [00:14<01:28,  1.22s/it] 15%|█▌        | 13/84 [00:15<01:26,  1.22s/it] 17%|█▋        | 14/84 [00:16<01:25,  1.23s/it] 18%|█▊        | 15/84 [00:18<01:24,  1.23s/it] 19%|█▉        | 16/84 [00:19<01:23,  1.23s/it] 20%|██        | 17/84 [00:20<01:22,  1.23s/it] 21%|██▏       | 18/84 [00:21<01:21,  1.23s/it] 23%|██▎       | 19/84 [00:23<01:19,  1.23s/it] 24%|██▍       | 20/84 [00:24<01:18,  1.23s/it] 25%|██▌       | 21/84 [00:25<01:17,  1.23s/it] 26%|██▌       | 22/84 [00:26<01:16,  1.23s/it] 27%|██▋       | 23/84 [00:28<01:15,  1.23s/it] 29%|██▊       | 24/84 [00:29<01:13,  1.23s/it] 30%|██▉       | 25/84 [00:30<01:12,  1.23s/it] 31%|███       | 26/84 [00:31<01:11,  1.23s/it] 32%|███▏      | 27/84 [00:32<01:10,  1.23s/it] 33%|███▎      | 28/84 [00:34<01:09,  1.23s/it] 35%|███▍      | 29/84 [00:35<01:07,  1.24s/it] 36%|███▌      | 30/84 [00:36<01:06,  1.24s/it] 37%|███▋      | 31/84 [00:37<01:05,  1.23s/it] 38%|███▊      | 32/84 [00:39<01:04,  1.23s/it] 39%|███▉      | 33/84 [00:40<01:03,  1.24s/it] 40%|████      | 34/84 [00:41<01:01,  1.24s/it] 42%|████▏     | 35/84 [00:42<01:00,  1.24s/it] 43%|████▎     | 36/84 [00:44<00:59,  1.23s/it] 44%|████▍     | 37/84 [00:45<00:58,  1.23s/it] 45%|████▌     | 38/84 [00:46<00:56,  1.24s/it] 46%|████▋     | 39/84 [00:47<00:55,  1.24s/it] 48%|████▊     | 40/84 [00:49<00:54,  1.24s/it] 49%|████▉     | 41/84 [00:50<00:53,  1.24s/it] 50%|█████     | 42/84 [00:51<00:52,  1.24s/it] 51%|█████     | 43/84 [00:52<00:50,  1.24s/it] 52%|█████▏    | 44/84 [00:53<00:49,  1.24s/it] 54%|█████▎    | 45/84 [00:55<00:48,  1.24s/it] 55%|█████▍    | 46/84 [00:56<00:47,  1.24s/it] 56%|█████▌    | 47/84 [00:57<00:45,  1.24s/it] 57%|█████▋    | 48/84 [00:58<00:44,  1.24s/it] 58%|█████▊    | 49/84 [01:00<00:43,  1.24s/it] 60%|█████▉    | 50/84 [01:01<00:42,  1.24s/it] 61%|██████    | 51/84 [01:02<00:40,  1.24s/it] 62%|██████▏   | 52/84 [01:03<00:39,  1.24s/it] 63%|██████▎   | 53/84 [01:05<00:38,  1.24s/it] 64%|██████▍   | 54/84 [01:06<00:37,  1.24s/it] 65%|██████▌   | 55/84 [01:07<00:35,  1.24s/it] 67%|██████▋   | 56/84 [01:08<00:34,  1.24s/it] 68%|██████▊   | 57/84 [01:10<00:33,  1.24s/it] 69%|██████▉   | 58/84 [01:11<00:32,  1.24s/it] 70%|███████   | 59/84 [01:12<00:30,  1.24s/it] 71%|███████▏  | 60/84 [01:13<00:29,  1.24s/it] 73%|███████▎  | 61/84 [01:15<00:28,  1.24s/it] 74%|███████▍  | 62/84 [01:16<00:27,  1.24s/it] 75%|███████▌  | 63/84 [01:17<00:26,  1.24s/it] 76%|███████▌  | 64/84 [01:18<00:24,  1.24s/it] 77%|███████▋  | 65/84 [01:20<00:23,  1.24s/it] 79%|███████▊  | 66/84 [01:21<00:22,  1.24s/it] 80%|███████▉  | 67/84 [01:22<00:21,  1.24s/it] 81%|████████  | 68/84 [01:23<00:19,  1.24s/it] 82%|████████▏ | 69/84 [01:24<00:18,  1.24s/it] 83%|████████▎ | 70/84 [01:26<00:17,  1.24s/it] 85%|████████▍ | 71/84 [01:27<00:16,  1.24s/it] 86%|████████▌ | 72/84 [01:28<00:14,  1.24s/it] 87%|████████▋ | 73/84 [01:29<00:13,  1.24s/it] 88%|████████▊ | 74/84 [01:31<00:12,  1.24s/it] 89%|████████▉ | 75/84 [01:32<00:11,  1.24s/it] 90%|█████████ | 76/84 [01:33<00:09,  1.24s/it] 92%|█████████▏| 77/84 [01:34<00:08,  1.24s/it] 93%|█████████▎| 78/84 [01:36<00:07,  1.24s/it] 94%|█████████▍| 79/84 [01:37<00:06,  1.24s/it] 95%|█████████▌| 80/84 [01:38<00:04,  1.24s/it] 96%|█████████▋| 81/84 [01:39<00:03,  1.24s/it] 98%|█████████▊| 82/84 [01:41<00:02,  1.24s/it] 99%|█████████▉| 83/84 [01:42<00:01,  1.24s/it] 99%|█████████▉| 83/84 [01:43<00:01,  1.24s/it]
Overwrite dataset info from restored data version if exists.
Original perplexity on wikitext = 5.109
04/29/2024 00:34:33 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/vashistt/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee
04/29/2024 00:34:33 - INFO - datasets.info - Loading Dataset info from /home/vashistt/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee
Found cached dataset gsm8k (/home/vashistt/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee)
04/29/2024 00:34:33 - INFO - datasets.builder - Found cached dataset gsm8k (/home/vashistt/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee)
Loading Dataset info from /home/vashistt/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee
04/29/2024 00:34:33 - INFO - datasets.info - Loading Dataset info from /home/vashistt/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee
Overwrite dataset info from restored data version if exists.
04/29/2024 00:34:34 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/vashistt/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee
04/29/2024 00:34:34 - INFO - datasets.info - Loading Dataset info from /home/vashistt/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee
Found cached dataset gsm8k (/home/vashistt/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee)
04/29/2024 00:34:34 - INFO - datasets.builder - Found cached dataset gsm8k (/home/vashistt/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee)
Loading Dataset info from /home/vashistt/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee
04/29/2024 00:34:34 - INFO - datasets.info - Loading Dataset info from /home/vashistt/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee
test: nsamples 1319
sample 0
sample 50
Traceback (most recent call last):
  File "/home/vashistt/Desktop/anlp-project/lora_ft/Run_evals.py", line 649, in <module>
    main()
  File "/home/vashistt/Desktop/anlp-project/lora_ft/Run_evals.py", line 542, in main
    og_ppl_gsm, og_runtime_gsm = evaluate_ppl('gsm8k', model, tokenizer, model.seqlen)
  File "/home/vashistt/Desktop/anlp-project/lora_ft/evaluate_ppl.py", line 58, in evaluate_ppl
    ppl = eval_ppl_test_gsm8k(model, testenc, device = model.device)
  File "/home/vashistt/Desktop/anlp-project/lora_ft/../lib/eval_combined.py", line 243, in eval_ppl_test_gsm8k
    outputs = model(input_ids, labels=target_ids)
  File "/home/vashistt/ENTER/envs/bonsai_pruning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/vashistt/ENTER/envs/bonsai_pruning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/vashistt/ENTER/envs/bonsai_pruning/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 687, in forward
    outputs = self.model(
  File "/home/vashistt/ENTER/envs/bonsai_pruning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/vashistt/ENTER/envs/bonsai_pruning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/vashistt/ENTER/envs/bonsai_pruning/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 577, in forward
    layer_outputs = decoder_layer(
  File "/home/vashistt/ENTER/envs/bonsai_pruning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/vashistt/ENTER/envs/bonsai_pruning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/vashistt/ENTER/envs/bonsai_pruning/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 289, in forward
    hidden_states = self.input_layernorm(hidden_states)
  File "/home/vashistt/ENTER/envs/bonsai_pruning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/vashistt/ENTER/envs/bonsai_pruning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/vashistt/ENTER/envs/bonsai_pruning/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 85, in forward
    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 44.32 GiB of which 19.75 MiB is free. Including non-PyTorch memory, this process has 44.29 GiB memory in use. Of the allocated memory 43.29 GiB is allocated by PyTorch, and 510.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
